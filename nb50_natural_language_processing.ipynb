{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50),\n",
    "         (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60),\n",
    "         (\"data science\", 60, 70), (\"analytics\", 90, 3),\n",
    "         (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0),\n",
    "         (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10),\n",
    "         (\"self-starter\", 30, 50), (\"customer focus\", 65, 15),\n",
    "         (\"thought leadership\", 35, 35)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def text_size(total: int) -> float:\n",
    "    \"\"\"equals 8 if total is 0, 28 if total is 200\"\"\"\n",
    "    return 8 + total / 200 * 20\n",
    "\n",
    "for word, job_popularity, resume_popularity in data:\n",
    "    plt.text(job_popularity, resume_popularity, word,\n",
    "                ha='center', va='center',\n",
    "                size=text_size(job_popularity + resume_popularity))\n",
    "plt.xlabel(\"Popularity on Job Postings\")\n",
    "plt.ylabel(\"Popularity on Resumes\")\n",
    "plt.axis([0, 100, 0, 100])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig('image/img025.png')\n",
    "plt.gca().clear()\n",
    "plt.close()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-Gram Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_unicode(text: str) -> str:\n",
    "    return text.replace(u\"\\u2019\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://www.oreilly.com/ideas/what-is-data-science\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "content = soup.find(\"div\", \"article-body\")   # find article-body div\n",
    "regex = r\"[\\w']+|[\\.]\"                       # matches a word or a period\n",
    "\n",
    "document = []\n",
    "\n",
    "for paragraph in content(\"p\"):\n",
    "    words = re.findall(regex, fix_unicode(paragraph.text))\n",
    "    document.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "transitions = defaultdict(list)\n",
    "for prev, current in zip(document, document[1:]):\n",
    "    transitions[prev].append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_bigrams() -> str:\n",
    "    current = \".\"   # this means the next word will start a sentence\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current]    # bigrams (current, _)\n",
    "        current = random.choice(next_word_candidates)  # choose one at random\n",
    "        result.append(current)                         # append it to results\n",
    "        if current == \".\": return \" \".join(result)     # if \".\" we're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_transitions = defaultdict(list)\n",
    "starts = []\n",
    "\n",
    "for prev, current, next in zip(document, document[1:], document[2:]):\n",
    "\n",
    "    if prev == \".\":              # if the previous \"word\" was a period\n",
    "        starts.append(current)   # then this is a start word\n",
    "\n",
    "    trigram_transitions[(prev, current)].append(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_trigrams() -> str:\n",
    "    current = random.choice(starts)   # choose a random starting word\n",
    "    prev = \".\"                        # and precede it with a '.'\n",
    "    result = [current]\n",
    "    while True:\n",
    "        next_word_candidates = trigram_transitions[(prev, current)]\n",
    "        next_word = random.choice(next_word_candidates)\n",
    "\n",
    "        prev, current = current, next_word\n",
    "        result.append(current)\n",
    "\n",
    "        if current == \".\":\n",
    "            return \" \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "# Type alias to refer to grammars later\n",
    "Grammar = Dict[str, List[str]]\n",
    "\n",
    "grammar = {\n",
    "    \"_S\"  : [\"_NP _VP\"],\n",
    "    \"_NP\" : [\"_N\",\n",
    "             \"_A _NP _P _A _N\"],\n",
    "    \"_VP\" : [\"_V\",\n",
    "             \"_V _NP\"],\n",
    "    \"_N\"  : [\"data science\", \"Python\", \"regression\"],\n",
    "    \"_A\"  : [\"big\", \"linear\", \"logistic\"],\n",
    "    \"_P\"  : [\"about\", \"near\"],\n",
    "    \"_V\"  : [\"learns\", \"trains\", \"tests\", \"is\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(token: str) -> bool:\n",
    "    return token[0] != \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(grammar: Grammar, tokens: List[str]) -> List[str]:\n",
    "    for i, token in enumerate(tokens):\n",
    "        # If this is a terminal token, skip it.\n",
    "        if is_terminal(token): continue\n",
    "\n",
    "        # Otherwise, it's a non-terminal token,\n",
    "        # so we need to choose a replacement at random.\n",
    "        replacement = random.choice(grammar[token])\n",
    "\n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            # Replacement could be e.g. \"_NP _VP\", so we need to\n",
    "            # split it on spaces and splice it in.\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "\n",
    "        # Now call expand on the new list of tokens.\n",
    "        return expand(grammar, tokens)\n",
    "\n",
    "    # If we get here we had all terminals and are done\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(grammar: Grammar) -> List[str]:\n",
    "    return expand(grammar, [\"_S\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Aside: Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import random\n",
    "\n",
    "def roll_a_die() -> int:\n",
    "    return random.choice([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "def direct_sample() -> Tuple[int, int]:\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_y_given_x(x: int) -> int:\n",
    "    \"\"\"equally likely to be x + 1, x + 2, ... , x + 6\"\"\"\n",
    "    return x + roll_a_die()\n",
    "\n",
    "def random_x_given_y(y: int) -> int:\n",
    "    if y <= 7:\n",
    "        # if the total is 7 or less, the first die is equally likely to be\n",
    "        # 1, 2, ..., (total - 1)\n",
    "        return random.randrange(1, y)\n",
    "    else:\n",
    "        # if the total is 7 or more, the first die is equally likely to be\n",
    "        # (total - 6), (total - 5), ..., 6\n",
    "        return random.randrange(y - 6, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sample(num_iters: int = 100) -> Tuple[int, int]:\n",
    "    x, y = 1, 2 # doesn't really matter\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distributions(num_samples: int = 1000) -> Dict[int, List[int]]:\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from(weights: List[float]) -> int:\n",
    "    \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()      # uniform between 0 and total\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                       # return the smallest i such that\n",
    "        if rnd <= 0: return i          # weights[0] + ... + weights[i] >= rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Draw 1000 times and count\n",
    "draws = Counter(sample_from([0.1, 0.1, 0.8]) for _ in range(1000))\n",
    "assert 10 < draws[0] < 190   # should be ~10%, this is a really loose test\n",
    "assert 10 < draws[1] < 190   # should be ~10%, this is a really loose test\n",
    "assert 650 < draws[2] < 950  # should be ~80%, this is a really loose test\n",
    "assert draws[0] + draws[1] + draws[2] == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]\n",
    "\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of Counters, one for each document\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "# a list of Counters, one for each topic\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# a list of numbers, one for each topic\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# a list of numbers, one for each document\n",
    "document_lengths = [len(document) for document in documents]\n",
    "\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic: int, d: int, alpha: float = 0.1) -> float:\n",
    "    \"\"\"\n",
    "    The fraction of words in document _d_\n",
    "    that are assigned to _topic_ (plus some smoothing)\n",
    "    \"\"\"\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word: str, topic: int, beta: float = 0.1) -> float:\n",
    "    \"\"\"\n",
    "    The fraction of words assigned to _topic_\n",
    "    that equal _word_ (plus some smoothing)\n",
    "    \"\"\"\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_weight(d: int, word: str, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Given a document and a word in that document,\n",
    "    return the weight for the kth topic\n",
    "    \"\"\"\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d: int, word: str) -> int:\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "for iter in tqdm.trange(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(k, word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = [\"Big Data and programming languages\",\n",
    "               \"Python and statistics\",\n",
    "               \"databases\",\n",
    "               \"machine learning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document, topic_counts in zip(documents, document_topic_counts):\n",
    "    print(document)\n",
    "    for topic, count in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic_names[topic], count)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from common.dot import dot\n",
    "from common.vector_type import Vector\n",
    "\n",
    "def cosine_similarity(v1: Vector, v2: Vector) -> float:\n",
    "    return dot(v1, v2) / math.sqrt(dot(v1, v1) * dot(v2, v2))\n",
    "\n",
    "assert cosine_similarity([1., 1, 1], [2., 2, 2]) == 1, \"same direction\"\n",
    "assert cosine_similarity([-1., -1], [2., 2]) == -1,    \"opposite direction\"\n",
    "assert cosine_similarity([1., 0], [0., 1]) == 0,       \"orthogonal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"\"]\n",
    "nouns = [\"bed\", \"car\", \"boat\", \"cat\"]\n",
    "verbs = [\"is\", \"was\", \"seems\"]\n",
    "adverbs = [\"very\", \"quite\", \"extremely\", \"\"]\n",
    "adjectives = [\"slow\", \"fast\", \"soft\", \"hard\"]\n",
    "\n",
    "def make_sentence() -> str:\n",
    "    return \" \".join([\n",
    "        \"The\",\n",
    "        random.choice(colors),\n",
    "        random.choice(nouns),\n",
    "        random.choice(verbs),\n",
    "        random.choice(adverbs),\n",
    "        random.choice(adjectives),\n",
    "        \".\"\n",
    "    ])\n",
    "\n",
    "NUM_SENTENCES = 50\n",
    "\n",
    "random.seed(0)\n",
    "sentences = [make_sentence() for _ in range(NUM_SENTENCES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.tensor import Tensor\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, words: List[str] = None) -> None:\n",
    "        self.w2i: Dict[str, int] = {}  # mapping word -> word_id\n",
    "        self.i2w: Dict[int, str] = {}  # mapping word_id -> word\n",
    "\n",
    "        for word in (words or []):     # If words were provided,\n",
    "            self.add(word)             # add them.\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        \"\"\"how many words are in the vocabulary\"\"\"\n",
    "        return len(self.w2i)\n",
    "\n",
    "    def add(self, word: str) -> None:\n",
    "        if word not in self.w2i:        # If the word is new to us:\n",
    "            word_id = len(self.w2i)     # Find the next id.\n",
    "            self.w2i[word] = word_id    # Add to the word -> word_id map.\n",
    "            self.i2w[word_id] = word    # Add to the word_id -> word map.\n",
    "\n",
    "    def get_id(self, word: str) -> int:\n",
    "        \"\"\"return the id of the word (or None)\"\"\"\n",
    "        return self.w2i.get(word)\n",
    "\n",
    "    def get_word(self, word_id: int) -> str:\n",
    "        \"\"\"return the word with the given id (or None)\"\"\"\n",
    "        return self.i2w.get(word_id)\n",
    "\n",
    "    def one_hot_encode(self, word: str) -> Tensor:\n",
    "        word_id = self.get_id(word)\n",
    "        assert word_id is not None, f\"unknown word {word}\"\n",
    "\n",
    "        return [1.0 if i == word_id else 0.0 for i in range(self.size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary([\"a\", \"b\", \"c\"])\n",
    "assert vocab.size == 3,              \"there are 3 words in the vocab\"\n",
    "assert vocab.get_id(\"b\") == 1,       \"b should have word_id 1\"\n",
    "assert vocab.one_hot_encode(\"b\") == [0, 1, 0]\n",
    "assert vocab.get_id(\"z\") is None,    \"z is not in the vocab\"\n",
    "assert vocab.get_word(2) == \"c\",     \"word_id 2 should be c\"\n",
    "vocab.add(\"z\")\n",
    "assert vocab.size == 4,              \"now there are 4 words in the vocab\"\n",
    "assert vocab.get_id(\"z\") == 3,       \"now z should have id 3\"\n",
    "assert vocab.one_hot_encode(\"z\") == [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_vocab(vocab: Vocabulary, filename: str) -> None:\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(vocab.w2i, f)       # Only need to save w2i\n",
    "\n",
    "def load_vocab(filename: str) -> Vocabulary:\n",
    "    vocab = Vocabulary()\n",
    "    with open(filename) as f:\n",
    "        # Load w2i and generate i2w from it.\n",
    "        vocab.w2i = json.load(f)\n",
    "        vocab.i2w = {id: word for word, id in vocab.w2i.items()}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "from common.layer import Layer\n",
    "from common.random_tensor import random_tensor\n",
    "from common.zeros_like import zeros_like\n",
    "\n",
    "class Embedding(Layer):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int) -> None:\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # One vector of size embedding_dim for each desired embedding\n",
    "        self.embeddings = random_tensor(num_embeddings, embedding_dim)\n",
    "        self.grad = zeros_like(self.embeddings)\n",
    "\n",
    "        # Save last input id\n",
    "        self.last_input_id = None\n",
    "\n",
    "    def forward(self, input_id: int) -> Tensor:\n",
    "        \"\"\"Just select the embedding vector corresponding to the input id\"\"\"\n",
    "        self.input_id = input_id    # remember for use in backpropagation\n",
    "\n",
    "        return self.embeddings[input_id]\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> None:\n",
    "        # Zero out the gradient corresponding to the last input.\n",
    "        # This is way cheaper than creating a new all-zero tensor each time.\n",
    "        if self.last_input_id is not None:\n",
    "            zero_row = [0 for _ in range(self.embedding_dim)]\n",
    "            self.grad[self.last_input_id] = zero_row\n",
    "\n",
    "        self.last_input_id = self.input_id\n",
    "        self.grad[self.input_id] = gradient\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return [self.embeddings]\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return [self.grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(Embedding):\n",
    "    def __init__(self, vocab: Vocabulary, embedding_dim: int) -> None:\n",
    "        # Call the superclass constructor\n",
    "        super().__init__(vocab.size, embedding_dim)\n",
    "\n",
    "        # And hang onto the vocab\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __getitem__(self, word: str) -> Tensor:\n",
    "        word_id = self.vocab.get_id(word)\n",
    "        if word_id is not None:\n",
    "            return self.embeddings[word_id]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def closest(self, word: str, n: int = 5) -> List[Tuple[float, str]]:\n",
    "        \"\"\"Returns the n closest words based on cosine similarity\"\"\"\n",
    "        vector = self[word]\n",
    "\n",
    "        # Compute pairs (similarity, other_word), and sort most similar first\n",
    "        scores = [(cosine_similarity(vector, self.embeddings[i]), other_word)\n",
    "                  for other_word, i in self.vocab.w2i.items()]\n",
    "        scores.sort(reverse=True)\n",
    "\n",
    "        return scores[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# This is not a great regex, but it works on our data.\n",
    "tokenized_sentences = [re.findall(\"[a-z]+|[.]\", sentence.lower())\n",
    "                        for sentence in sentences]\n",
    "\n",
    "# Create a vocabulary (that is, a mapping word -> word_id) based on our text.\n",
    "vocab = Vocabulary(word\n",
    "                    for sentence_words in tokenized_sentences\n",
    "                    for word in sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs: List[int] = []\n",
    "targets: List[Tensor] = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for i, word in enumerate(sentence):          # For each word\n",
    "        for j in [i - 2, i - 1, i + 1, i + 2]:   # take the nearby locations\n",
    "            if 0 <= j < len(sentence):           # that aren't out of bounds\n",
    "                nearby_word = sentence[j]        # and get those words.\n",
    "\n",
    "                # Add an input that's the original word_id\n",
    "                inputs.append(vocab.get_id(word))\n",
    "\n",
    "                # Add a target that's the one-hot-encoded nearby word\n",
    "                targets.append(vocab.one_hot_encode(nearby_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for learning word vectors\n",
    "\n",
    "from common.linear import Linear\n",
    "from common.sequential import Sequential\n",
    "\n",
    "random.seed(0)\n",
    "EMBEDDING_DIM = 5  # seems like a good size\n",
    "\n",
    "# Define the embedding layer separately, so we can reference it.\n",
    "embedding = TextEmbedding(vocab=vocab, embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "model = Sequential([\n",
    "    # Given a word (as a vector of word_ids), look up its embedding.\n",
    "    embedding,\n",
    "    # And use a linear layer to compute scores for \"nearby words\".\n",
    "    Linear(input_dim=EMBEDDING_DIM, output_dim=vocab.size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the word vector model\n",
    "\n",
    "from common.soft_max_cross_entropy import SoftmaxCrossEntropy\n",
    "from common.gradient_descent import GradientDescent\n",
    "\n",
    "loss = SoftmaxCrossEntropy()\n",
    "optimizer = GradientDescent(learning_rate=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0.0\n",
    "    for input, target in zip(inputs, targets):\n",
    "        predicted = model.forward(input)\n",
    "        epoch_loss += loss.loss(predicted, target)\n",
    "        gradient = loss.gradient(predicted, target)\n",
    "        model.backward(gradient)\n",
    "        optimizer.step(model)\n",
    "    print(epoch, epoch_loss)            # Print the loss\n",
    "    print(embedding.closest(\"black\"))   # and also a few nearest words\n",
    "    print(embedding.closest(\"slow\"))    # so we can see what's being\n",
    "    print(embedding.closest(\"car\"))     # learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore most similar words\n",
    "\n",
    "pairs = [(cosine_similarity(embedding[w1], embedding[w2]), w1, w2)\n",
    "            for w1 in vocab.w2i\n",
    "            for w2 in vocab.w2i\n",
    "            if w1 < w2]\n",
    "pairs.sort(reverse=True)\n",
    "print(pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word vectors\n",
    "plt.close()\n",
    "\n",
    "from common.pca import pca\n",
    "from common.transform import transform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the first two principal components and transform the word vectors\n",
    "components = pca(embedding.embeddings, 2)\n",
    "transformed = transform(embedding.embeddings, components)\n",
    "\n",
    "# Scatter the points (and make them white so they're \"invisible\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(*zip(*transformed), marker='.', color='w')\n",
    "\n",
    "# Add annotations for each word at its transformed location\n",
    "for word, idx in vocab.w2i.items():\n",
    "    ax.annotate(word, transformed[idx])\n",
    "\n",
    "# And hide the axes\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig('image/img026.png')\n",
    "plt.gca().clear()\n",
    "plt.close()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.tensor_apply import tensor_apply\n",
    "from common.tanh import tanh\n",
    "\n",
    "class SimpleRnn(Layer):\n",
    "    \"\"\"Just about the simplest possible recurrent layer.\"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int) -> None:\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.w = random_tensor(hidden_dim, input_dim, init='xavier')\n",
    "        self.u = random_tensor(hidden_dim, hidden_dim, init='xavier')\n",
    "        self.b = random_tensor(hidden_dim)\n",
    "\n",
    "        self.reset_hidden_state()\n",
    "\n",
    "    def reset_hidden_state(self) -> None:\n",
    "        self.hidden = [0 for _ in range(self.hidden_dim)]\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        self.input = input              # Save both input and previous\n",
    "        self.prev_hidden = self.hidden  # hidden state to use in backprop.\n",
    "\n",
    "        a = [(dot(self.w[h], input) +           # weights @ input\n",
    "              dot(self.u[h], self.hidden) +     # weights @ hidden\n",
    "              self.b[h])                        # bias\n",
    "             for h in range(self.hidden_dim)]\n",
    "\n",
    "        self.hidden = tensor_apply(tanh, a)  # Apply tanh activation\n",
    "        return self.hidden                   # and return the result.\n",
    "\n",
    "    def backward(self, gradient: Tensor):\n",
    "        # Backpropagate through the tanh\n",
    "        a_grad = [gradient[h] * (1 - self.hidden[h] ** 2)\n",
    "                  for h in range(self.hidden_dim)]\n",
    "\n",
    "        # b has the same gradient as a\n",
    "        self.b_grad = a_grad\n",
    "\n",
    "        # Each w[h][i] is multiplied by input[i] and added to a[h],\n",
    "        # so each w_grad[h][i] = a_grad[h] * input[i]\n",
    "        self.w_grad = [[a_grad[h] * self.input[i]\n",
    "                        for i in range(self.input_dim)]\n",
    "                       for h in range(self.hidden_dim)]\n",
    "\n",
    "        # Each u[h][h2] is multiplied by hidden[h2] and added to a[h],\n",
    "        # so each u_grad[h][h2] = a_grad[h] * prev_hidden[h2]\n",
    "        self.u_grad = [[a_grad[h] * self.prev_hidden[h2]\n",
    "                        for h2 in range(self.hidden_dim)]\n",
    "                       for h in range(self.hidden_dim)]\n",
    "\n",
    "        # Each input[i] is multiplied by every w[h][i] and added to a[h],\n",
    "        # so each input_grad[i] = sum(a_grad[h] * w[h][i] for h in ...)\n",
    "        return [sum(a_grad[h] * self.w[h][i] for h in range(self.hidden_dim))\n",
    "                for i in range(self.input_dim)]\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return [self.w, self.u, self.b]\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return [self.w_grad, self.u_grad, self.b_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Using a Character-Level RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://www.ycombinator.com/topcompanies/\"\n",
    "soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
    "\n",
    "# We get the companies twice, so use a set comprehension to deduplicate.\n",
    "companies = list({b.text\n",
    "                    for b in soup(\"b\")\n",
    "                    if \"h4\" in b.get(\"class\", ())})\n",
    "assert len(companies) == 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary([c for company in companies for c in company])\n",
    "\n",
    "START = \"^\"\n",
    "STOP = \"$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add them to the vocabulary too.\n",
    "vocab.add(START)\n",
    "vocab.add(STOP)\n",
    "\n",
    "HIDDEN_DIM = 32  # You should experiment with different sizes!\n",
    "\n",
    "rnn1 =  SimpleRnn(input_dim=vocab.size, hidden_dim=HIDDEN_DIM)\n",
    "rnn2 =  SimpleRnn(input_dim=HIDDEN_DIM, hidden_dim=HIDDEN_DIM)\n",
    "linear = Linear(input_dim=HIDDEN_DIM, output_dim=vocab.size)\n",
    "\n",
    "model = Sequential([\n",
    "    rnn1,\n",
    "    rnn2,\n",
    "    linear\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.softmax import softmax\n",
    "\n",
    "def generate(seed: str = START, max_len: int = 50) -> str:\n",
    "    rnn1.reset_hidden_state()  # Reset both hidden states.\n",
    "    rnn2.reset_hidden_state()\n",
    "    output = [seed]            # Start the output with the specified seed.\n",
    "\n",
    "    # Keep going until we produce the STOP character or reach the max length\n",
    "    while output[-1] != STOP and len(output) < max_len:\n",
    "        # Use the last character as the input\n",
    "        input = vocab.one_hot_encode(output[-1])\n",
    "\n",
    "        # Generate scores using the model\n",
    "        predicted = model.forward(input)\n",
    "\n",
    "        # Convert them to probabilities and draw a random char_id\n",
    "        probabilities = softmax(predicted)\n",
    "        next_char_id = sample_from(probabilities)\n",
    "\n",
    "        # Add the corresponding char to our output\n",
    "        output.append(vocab.get_word(next_char_id))\n",
    "\n",
    "    # Get rid of START and END characters and return the word.\n",
    "    return ''.join(output[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.momentum import Momentum\n",
    "\n",
    "loss = SoftmaxCrossEntropy()\n",
    "optimizer = Momentum(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(300):\n",
    "    random.shuffle(companies)  # Train in a different order each epoch.\n",
    "    epoch_loss = 0             # Track the loss.\n",
    "    for company in tqdm.tqdm(companies):\n",
    "        rnn1.reset_hidden_state()  # Reset both hidden states.\n",
    "        rnn2.reset_hidden_state()\n",
    "        company = START + company + STOP   # Add START and STOP characters.\n",
    "\n",
    "        # The rest is just our usual training loop, except that the inputs\n",
    "        # and target are the one-hot-encoded previous and next characters.\n",
    "        for prev, next in zip(company, company[1:]):\n",
    "            input = vocab.one_hot_encode(prev)\n",
    "            target = vocab.one_hot_encode(next)\n",
    "            predicted = model.forward(input)\n",
    "            epoch_loss += loss.loss(predicted, target)\n",
    "            gradient = loss.gradient(predicted, target)\n",
    "            model.backward(gradient)\n",
    "            optimizer.step(model)\n",
    "\n",
    "    # Each epoch print the loss and also generate a name\n",
    "    print(epoch, epoch_loss, generate())\n",
    "\n",
    "    # Turn down the learning rate for the last 100 epochs.\n",
    "    # There's no principled reason for this, but it seems to work.\n",
    "    if epoch == 200:\n",
    "        optimizer.lr *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.optimizer import Optimizer\n",
    "\n",
    "class EmbeddingOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    Optimized for the case where there are\n",
    "    only embedding layers with single id updates.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate: float) -> None:\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        for param, grad in zip(layer.params(), layer.grads()):\n",
    "            # Find the first (only) row with nonzero values.\n",
    "            for idx, row in enumerate(grad):\n",
    "                if row[0] != 0:\n",
    "                    break\n",
    "\n",
    "            # Then update just that row.\n",
    "            for j in range(len(row)):\n",
    "                param[idx][j] -= grad[idx][j] * self.lr"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "580334040f56039d3caacea6cc9b85b0f989c2f977ea3f5715f6ed60c64ca9a7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
