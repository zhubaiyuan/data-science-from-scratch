{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "Tensor = list\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "assert shape([1, 2, 3]) == [3]\n",
    "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor.\n",
    "    Otherwise, tensor is 1-dimensonal (that is, a vector).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "assert is_1d([1, 2, 3])\n",
    "assert not is_1d([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results.\n",
    "\n",
    "assert tensor_sum([1, 2, 3]) == 6\n",
    "assert tensor_sum([[1, 2], [3, 4]]) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f elementwise\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
    "\n",
    "assert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4]\n",
    "assert tensor_apply(lambda x: 2 * x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "assert zeros_like([1, 2, 3]) == [0, 0, 0]\n",
    "assert zeros_like([[1, 2], [3, 4]]) == [[0, 0], [0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "\n",
    "import operator\n",
    "assert tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n",
    "assert tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]) == [4, 10, 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Layer Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Our neural networks will be composed of Layers, each of which\n",
    "    knows how to do some computation on its inputs in the \"forward\"\n",
    "    direction and propagate gradients in the \"backward\" direction.\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Note the lack of types. We're not going to be prescriptive\n",
    "        about what kinds of inputs layers can take and what kinds\n",
    "        of outputs they can return.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Similarly, we're not going to be prescriptive about what the\n",
    "        gradient looks like. It's up to you the user to make sure\n",
    "        that you're doing things sensibly.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the parameters of this layer. The default implementation\n",
    "        returns nothing, so that if you have a layer with no parameters\n",
    "        you don't have to implement this.\n",
    "        \"\"\"\n",
    "        return ()\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the gradients, in the same order as params()\n",
    "        \"\"\"\n",
    "        return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.sigmoid import sigmoid\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply sigmoid to each element of the input tensor,\n",
    "        and save the results to use in backpropagation.\n",
    "        \"\"\"\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
    "                              self.sigmoids,\n",
    "                              gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from common.inverse_normal_cdf import inverse_normal_cdf\n",
    "\n",
    "def random_uniform(*dims: int) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [random.random() for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
    "\n",
    "def random_normal(*dims: int,\n",
    "                  mean: float = 0.0,\n",
    "                  variance: float = 1.0) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [mean + variance * inverse_normal_cdf(random.random())\n",
    "                for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
    "                for _ in range(dims[0])]\n",
    "\n",
    "assert shape(random_uniform(2, 3, 4)) == [2, 3, 4]\n",
    "assert shape(random_normal(5, 6, mean=10)) == [5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
    "    if init == 'normal':\n",
    "        return random_normal(*dims)\n",
    "    elif init == 'uniform':\n",
    "        return random_uniform(*dims)\n",
    "    elif init == 'xavier':\n",
    "        variance = len(dims) / sum(dims)\n",
    "        return random_normal(*dims, variance=variance)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown init: {init}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.dot import dot\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier') -> None:\n",
    "        \"\"\"\n",
    "        A layer of output_dim neurons, each with input_dim weights\n",
    "        (and a bias).\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # self.w[o] is the weights for the o-th neuron\n",
    "        self.w = random_tensor(output_dim, input_dim, init=init)\n",
    "\n",
    "        # self.b[o] is the bias term for the o-th neuron\n",
    "        self.b = random_tensor(output_dim, init=init)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Save the input to use in the backward pass.\n",
    "        self.input = input\n",
    "\n",
    "        # Return the vector of neuron outputs.\n",
    "        return [dot(input, self.w[o]) + self.b[o]\n",
    "                for o in range(self.output_dim)]\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        # Each b[o] gets added to output[o], which means\n",
    "        # the gradient of b is the same as the output gradient.\n",
    "        self.b_grad = gradient\n",
    "\n",
    "        # Each w[o][i] multiplies input[i] and gets added to output[o].\n",
    "        # So its gradient is input[i] * gradient[o].\n",
    "        self.w_grad = [[self.input[i] * gradient[o]\n",
    "                        for i in range(self.input_dim)]\n",
    "                       for o in range(self.output_dim)]\n",
    "\n",
    "        # Each input[i] multiplies every w[o][i] and gets added to every\n",
    "        # output[o]. So its gradient is the sum of w[o][i] * gradient[o]\n",
    "        # across all the outputs.\n",
    "        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n",
    "                for i in range(self.input_dim)]\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return [self.w, self.b]\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return [self.w_grad, self.b_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks as a Sequence of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \"\"\"\n",
    "    A layer consisting of a sequence of other layers.\n",
    "    It's up to you to make sure that the output of each layer\n",
    "    makes sense as the input to the next layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: List[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Just forward the input through the layers in order.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "        return gradient\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the params from each layer.\"\"\"\n",
    "        return (param for layer in self.layers for param in layer.params())\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the grads from each layer.\"\"\"\n",
    "        return (grad for layer in self.layers for grad in layer.grads())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        \"\"\"How does the loss change as the predictions change?\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSE(Loss):\n",
    "    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        # Compute the tensor of squared differences\n",
    "        squared_errors = tensor_combine(\n",
    "            lambda predicted, actual: (predicted - actual) ** 2,\n",
    "            predicted,\n",
    "            actual)\n",
    "\n",
    "        # And just add them up\n",
    "        return tensor_sum(squared_errors)\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda predicted, actual: 2 * (predicted - actual),\n",
    "            predicted,\n",
    "            actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_loss = SSE()\n",
    "assert sse_loss.loss([1, 2, 3], [10, 20, 30]) == 9 ** 2 + 18 ** 2 + 27 ** 2\n",
    "assert sse_loss.gradient([1, 2, 3], [10, 20, 30]) == [-18, -36, -54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    An optimizer updates the weights of a layer (in place) using information\n",
    "    known by either the layer or the optimizer (or by both).\n",
    "    \"\"\"\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.1) -> None:\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        for param, grad in zip(layer.params(), layer.grads()):\n",
    "            # Update param using a gradient step\n",
    "            param[:] = tensor_combine(\n",
    "                lambda param, grad: param - grad * self.lr,\n",
    "                param,\n",
    "                grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = [[1, 2], [3, 4]]\n",
    "\n",
    "for row in tensor:\n",
    "    row = [0, 0]\n",
    "assert tensor == [[1, 2], [3, 4]], \"assignment doesn't update a list\"\n",
    "\n",
    "for row in tensor:\n",
    "    row[:] = [0, 0]\n",
    "assert tensor == [[0, 0], [0, 0]], \"but slice assignment does\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum(Optimizer):\n",
    "    def __init__(self,\n",
    "                 learning_rate: float,\n",
    "                 momentum: float = 0.9) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.mo = momentum\n",
    "        self.updates: List[Tensor] = []  # running average\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        # If we have no previous updates, start with all zeros.\n",
    "        if not self.updates:\n",
    "            self.updates = [zeros_like(grad) for grad in layer.grads()]\n",
    "\n",
    "        for update, param, grad in zip(self.updates,\n",
    "                                       layer.params(),\n",
    "                                       layer.grads()):\n",
    "            # Apply momentum\n",
    "            update[:] = tensor_combine(\n",
    "                lambda u, g: self.mo * u + (1 - self.mo) * g,\n",
    "                update,\n",
    "                grad)\n",
    "\n",
    "            # Then take a gradient step\n",
    "            param[:] = tensor_combine(\n",
    "                lambda p, u: p - self.lr * u,\n",
    "                param,\n",
    "                update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: XOR Revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "ys = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "net = Sequential([\n",
    "    Linear(input_dim=2, output_dim=2),\n",
    "    Sigmoid(),\n",
    "    Linear(input_dim=2, output_dim=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "optimizer = GradientDescent(learning_rate=0.1)\n",
    "loss = SSE()\n",
    "\n",
    "with tqdm.trange(3000) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = net.forward(x)\n",
    "            epoch_loss += loss.loss(predicted, y)\n",
    "            gradient = loss.gradient(predicted, y)\n",
    "            net.backward(gradient)\n",
    "\n",
    "            optimizer.step(net)\n",
    "\n",
    "        t.set_description(f\"xor loss {epoch_loss:.3f}\")\n",
    "\n",
    "for param in net.params():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tanh(x: float) -> float:\n",
    "    # If x is very large or very small, tanh is (essentially) 1 or -1.\n",
    "    # We check for this because e.g. math.exp(1000) raises an error.\n",
    "    if x < -100:  return -1\n",
    "    elif x > 100: return 1\n",
    "\n",
    "    em2x = math.exp(-2 * x)\n",
    "    return (1 - em2x) / (1 + em2x)\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Save tanh output to use in backward pass.\n",
    "        self.tanh = tensor_apply(tanh, input)\n",
    "        return self.tanh\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda tanh, grad: (1 - tanh ** 2) * grad,\n",
    "            self.tanh,\n",
    "            gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        self.input = input\n",
    "        return tensor_apply(lambda x: max(x, 0), input)\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda x, grad: grad if x > 0 else 0,\n",
    "                              self.input,\n",
    "                              gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: FizzBuzz Revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.binary_encode import binary_encode\n",
    "from common.fizz_buzz_encode import fizz_buzz_encode\n",
    "\n",
    "xs = [binary_encode(n) for n in range(101, 1024)]\n",
    "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n",
    "\n",
    "NUM_HIDDEN = 25\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "net = Sequential([\n",
    "    Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
    "    Tanh(),\n",
    "    Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform'),\n",
    "    Sigmoid()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.argmax import argmax\n",
    "\n",
    "def fizzbuzz_accuracy(low: int, hi: int, net: Layer) -> float:\n",
    "    num_correct = 0\n",
    "    for n in range(low, hi):\n",
    "        x = binary_encode(n)\n",
    "        predicted = argmax(net.forward(x))\n",
    "        actual = argmax(fizz_buzz_encode(n))\n",
    "        if predicted == actual:\n",
    "            num_correct += 1\n",
    "\n",
    "    return num_correct / (hi - low)\n",
    "\n",
    "optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
    "loss = SSE()\n",
    "\n",
    "with tqdm.trange(1000) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = net.forward(x)\n",
    "            epoch_loss += loss.loss(predicted, y)\n",
    "            gradient = loss.gradient(predicted, y)\n",
    "            net.backward(gradient)\n",
    "\n",
    "            optimizer.step(net)\n",
    "\n",
    "        accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
    "        t.set_description(f\"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}\")\n",
    "\n",
    "# Now check results on the test set\n",
    "print(\"test results\", fizzbuzz_accuracy(1, 101, net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmaxes and Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Softmax along the last dimension\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        # Subtract largest value for numerical stabilitity.\n",
    "        largest = max(tensor)\n",
    "        exps = [math.exp(x - largest) for x in tensor]\n",
    "\n",
    "        sum_of_exps = sum(exps)                 # This is the total \"weight\".\n",
    "        return [exp_i / sum_of_exps             # Probability is the fraction\n",
    "                for exp_i in exps]              # of the total weight.\n",
    "    else:\n",
    "        return [softmax(tensor_i) for tensor_i in tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy(Loss):\n",
    "    \"\"\"\n",
    "    This is the negative-log-likelihood of the observed values, given the\n",
    "    neural net model. So if we choose weights to minimize it, our model will\n",
    "    be maximizing the likelihood of the observed data.\n",
    "    \"\"\"\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        # This will be log p_i for the actual class i and 0 for the other\n",
    "        # classes. We add a tiny amount to p to avoid taking log(0).\n",
    "        likelihoods = tensor_combine(lambda p, act: math.log(p + 1e-30) * act,\n",
    "                                     probabilities,\n",
    "                                     actual)\n",
    "\n",
    "        # And then we just sum up the negatives.\n",
    "        return -tensor_sum(likelihoods)\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        # Isn't this a pleasant equation?\n",
    "        return tensor_combine(lambda p, actual: p - actual,\n",
    "                              probabilities,\n",
    "                              actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "net = Sequential([\n",
    "    Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
    "    Tanh(),\n",
    "    Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform')\n",
    "    # No final sigmoid layer now\n",
    "])\n",
    "\n",
    "optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
    "loss = SoftmaxCrossEntropy()\n",
    "\n",
    "with tqdm.trange(100) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = net.forward(x)\n",
    "            epoch_loss += loss.loss(predicted, y)\n",
    "            gradient = loss.gradient(predicted, y)\n",
    "            net.backward(gradient)\n",
    "\n",
    "            optimizer.step(net)\n",
    "\n",
    "        accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
    "        t.set_description(f\"fb loss: {epoch_loss:.3f} acc: {accuracy:.2f}\")\n",
    "\n",
    "# Again check results on the test set\n",
    "print(\"test results\", fizzbuzz_accuracy(1, 101, net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, p: float) -> None:\n",
    "        self.p = p\n",
    "        self.train = True\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        if self.train:\n",
    "            # Create a mask of 0s and 1s shaped like the input\n",
    "            # using the specified probability.\n",
    "            self.mask = tensor_apply(\n",
    "                lambda _: 0 if random.random() < self.p else 1,\n",
    "                input)\n",
    "            # Multiply by the mask to dropout inputs.\n",
    "            return tensor_combine(operator.mul, input, self.mask)\n",
    "        else:\n",
    "            # During evaluation just scale down the outputs uniformly.\n",
    "            return tensor_apply(lambda x: x * (1 - self.p), input)\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        if self.train:\n",
    "            # Only propagate the gradients where mask == 1\n",
    "            return tensor_combine(operator.mul, gradient, self.mask)\n",
    "        else:\n",
    "            raise RuntimeError(\"don't call backward when not in train mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "    \n",
    "# This will download the data, change this to where you want it.\n",
    "# (Yes, it's a 0-argument function, that's what the library expects.)\n",
    "# (Yes, I'm assigning a lambda to a variable, like I said never to do.)\n",
    "mnist.temporary_dir = lambda: '/tmp'\n",
    "\n",
    "# Each of these functions first downloads the data and returns a numpy array.\n",
    "# We call .tolist() because our \"tensors\" are just lists.\n",
    "train_images = mnist.train_images().tolist()\n",
    "train_labels = mnist.train_labels().tolist()\n",
    "\n",
    "assert shape(train_images) == [60000, 28, 28]\n",
    "assert shape(train_labels) == [60000]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(10, 10)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        # Plot each image in black and white and hide the axes.\n",
    "        ax[i][j].imshow(train_images[10 * i + j], cmap='Greys')\n",
    "        ax[i][j].xaxis.set_visible(False)\n",
    "        ax[i][j].yaxis.set_visible(False)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig('image/img021.png')\n",
    "plt.gca().clear()\n",
    "plt.close()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST test data\n",
    "\n",
    "test_images = mnist.test_images().tolist()\n",
    "test_labels = mnist.test_labels().tolist()\n",
    "\n",
    "assert shape(test_images) == [10000, 28, 28]\n",
    "assert shape(test_labels) == [10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average pixel value\n",
    "avg = tensor_sum(train_images) / 60000 / 28 / 28\n",
    "\n",
    "# Recenter, rescale, and flatten\n",
    "train_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
    "                for image in train_images]\n",
    "test_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
    "                for image in test_images]\n",
    "\n",
    "assert shape(train_images) == [60000, 784], \"images should be flattened\"\n",
    "assert shape(test_images) == [10000, 784], \"images should be flattened\"\n",
    "\n",
    "# After centering, average pixel should be very close to 0\n",
    "assert -0.0001 < tensor_sum(train_images) < 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(i: int, num_labels: int = 10) -> List[float]:\n",
    "    return [1.0 if j == i else 0.0 for j in range(num_labels)]\n",
    "\n",
    "assert one_hot_encode(3) == [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "assert one_hot_encode(2, num_labels=5) == [0, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the test data\n",
    "\n",
    "train_labels = [one_hot_encode(label) for label in train_labels]\n",
    "test_labels = [one_hot_encode(label) for label in test_labels]\n",
    "\n",
    "assert shape(train_labels) == [60000, 10]\n",
    "assert shape(test_labels) == [10000, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop \n",
    "import tqdm\n",
    "\n",
    "def loop(model: Layer,\n",
    "            images: List[Tensor],\n",
    "            labels: List[Tensor],\n",
    "            loss: Loss,\n",
    "            optimizer: Optimizer = None) -> None:\n",
    "    correct = 0         # Track number of correct predictions.\n",
    "    total_loss = 0.0    # Track total loss.\n",
    "\n",
    "    with tqdm.trange(len(images)) as t:\n",
    "        for i in t:\n",
    "            predicted = model.forward(images[i])             # Predict.\n",
    "            if argmax(predicted) == argmax(labels[i]):       # Check for\n",
    "                correct += 1                                 # correctness.\n",
    "            total_loss += loss.loss(predicted, labels[i])    # Compute loss.\n",
    "\n",
    "            # If we're training, backpropagate gradient and update weights.\n",
    "            if optimizer is not None:\n",
    "                gradient = loss.gradient(predicted, labels[i])\n",
    "                model.backward(gradient)\n",
    "                optimizer.step(model)\n",
    "\n",
    "            # And update our metrics in the progress bar.\n",
    "            avg_loss = total_loss / (i + 1)\n",
    "            acc = correct / (i + 1)\n",
    "            t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logistic regression model for MNIST\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Logistic regression is just a linear layer followed by softmax\n",
    "model = Linear(784, 10)\n",
    "loss = SoftmaxCrossEntropy()\n",
    "\n",
    "# This optimizer seems to work\n",
    "optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
    "\n",
    "# Train on the training data\n",
    "loop(model, train_images, train_labels, loss, optimizer)\n",
    "\n",
    "# Test on the test data (no optimizer means just evaluate)\n",
    "loop(model, test_images, test_labels, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A deep neural network for MNIST\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Name them so we can turn train on and off\n",
    "dropout1 = Dropout(0.1)\n",
    "dropout2 = Dropout(0.1)\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(784, 30),  # Hidden layer 1: size 30\n",
    "    dropout1,\n",
    "    Tanh(),\n",
    "    Linear(30, 10),   # Hidden layer 2: size 10\n",
    "    dropout2,\n",
    "    Tanh(),\n",
    "    Linear(10, 10)    # Output layer: size 10\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the deep model for MNIST\n",
    "\n",
    "optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
    "loss = SoftmaxCrossEntropy()\n",
    "\n",
    "# Enable dropout and train (takes > 20 minutes on my laptop!)\n",
    "dropout1.train = dropout2.train = True\n",
    "loop(model, train_images, train_labels, loss, optimizer)\n",
    "\n",
    "# Disable dropout and evaluate\n",
    "dropout1.train = dropout2.train = False\n",
    "loop(model, test_images, test_labels, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_weights(model: Layer, filename: str) -> None:\n",
    "    weights = list(model.params())\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model: Layer, filename: str) -> None:\n",
    "    with open(filename) as f:\n",
    "        weights = json.load(f)\n",
    "\n",
    "    # Check for consistency\n",
    "    assert all(shape(param) == shape(weight)\n",
    "               for param, weight in zip(model.params(), weights))\n",
    "\n",
    "    # Then load using slice assignment:\n",
    "    for param, weight in zip(model.params(), weights):\n",
    "        param[:] = weight"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "580334040f56039d3caacea6cc9b85b0f989c2f977ea3f5715f6ed60c64ca9a7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
